{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2104,"status":"ok","timestamp":1702659013968,"user":{"displayName":"Alessio Desogus","userId":"05061804989687486718"},"user_tz":-60},"id":"sDuFL-wyX31i","outputId":"25949047-4a4e-4169-945a-296565f4e10a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIUNrwh9YXnR"},"outputs":[],"source":["pip install git+https://github.com/m-bain/whisperx.git"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24160,"status":"ok","timestamp":1702659058295,"user":{"displayName":"Alessio Desogus","userId":"05061804989687486718"},"user_tz":-60},"id":"e7HK0G-wXCKE","outputId":"820ef980-2a6f-42c3-9325-6ac03abb06d9"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.10/dist-packages/pyannote/audio/core/io.py:43: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n","  torchaudio.set_audio_backend(\"soundfile\")\n","/usr/local/lib/python3.10/dist-packages/torch_audiomentations/utils/io.py:27: UserWarning: torchaudio._backend.set_audio_backend has been deprecated. With dispatcher enabled, this function is no-op. You can remove the function call.\n","  torchaudio.set_audio_backend(\"soundfile\")\n"]}],"source":["import whisperx\n","import os\n","import json"]},{"cell_type":"markdown","metadata":{"id":"1NY7y41xXCKG"},"source":["# WhisperX Audio Transcription and Diarization"]},{"cell_type":"markdown","metadata":{"id":"OsCECUNZXCKI"},"source":["## Parameters Initialization"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1702659058295,"user":{"displayName":"Alessio Desogus","userId":"05061804989687486718"},"user_tz":-60},"id":"jrL9eZvrXCKI"},"outputs":[],"source":["device = \"cuda\"                     # device = \"cpu\" for running locally | device = \"cuda\" for running in google colab\n","batch_size = 16                     # reduce if low on GPU memory\n","compute_type = \"float16\"            # change to \"int8\" if low on GPU memory (may reduce accuracy)\n","language = \"de\"                     # german language"]},{"cell_type":"markdown","metadata":{"id":"SEOMNEXDXCKJ"},"source":["## Loading the Large-v2 model from WhisperX"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21520,"status":"ok","timestamp":1702659079813,"user":{"displayName":"Alessio Desogus","userId":"05061804989687486718"},"user_tz":-60},"id":"aQ0hw-gaXCKJ","outputId":"b015be66-ca36-4361-a03f-00542ef229aa"},"outputs":[{"name":"stderr","output_type":"stream","text":["INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.1.2. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/whisperx-vad-segmentation.bin`\n"]},{"name":"stdout","output_type":"stream","text":["Model was trained with pyannote.audio 0.0.1, yours is 3.1.0. Bad things might happen unless you revert pyannote.audio to 0.x.\n","Model was trained with torch 1.10.0+cu102, yours is 2.1.0+cu121. Bad things might happen unless you revert torch to 1.x.\n"]}],"source":["# loading the large-v2 model with german language specified\n","model = whisperx.load_model(\"large-v2\", device, compute_type=compute_type, language=language)"]},{"cell_type":"markdown","metadata":{"id":"Glf1BnA-XCKK"},"source":["## Transcribing, Aligning & Diarization of the Audio with the Original Whisper (batched)"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1702659079814,"user":{"displayName":"Alessio Desogus","userId":"05061804989687486718"},"user_tz":-60},"id":"RzcwVd_uXCKK"},"outputs":[],"source":["def transcribe_audio_diarization(device, audio_file, batch_size) :\n","    \"\"\"Load the audio file and return its transcribe with the alignment\"\"\"\n","\n","    # 1. transcribing the audio before alignment\n","    audio = whisperx.load_audio(audio_file)\n","    result_before_align = model.transcribe(audio, batch_size=batch_size, language=\"de\")\n","\n","    # 2. transcribing the audio after alignment\n","    model_a, metadata = whisperx.load_align_model(language_code=result_before_align[\"language\"], device=device)\n","    result_after_align = whisperx.align(result_before_align[\"segments\"], model_a, metadata, audio, device, return_char_alignments=False)\n","\n","    # 3. assign speaker labels\n","    diarize_model = whisperx.DiarizationPipeline(use_auth_token=YOUR_HF_TOKEN, device=device)\n","\n","    # 4. add min/max number of speakers if known diarize_model(audio, min_speakers=min_speakers, max_speakers=max_speakers)\n","    diarize_segments = diarize_model(audio, min_speakers=2, max_speakers=5)\n","\n","    # 5. final result : transcript with alignment and diarization\n","    result = whisperx.assign_word_speakers(diarize_segments, result_after_align)\n","\n","    return result"]},{"cell_type":"markdown","metadata":{"id":"f2exmvvwXCKL"},"source":["## Creating a JSON File for each Transcript"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Cimj3hiyXCKL"},"outputs":[],"source":["base_path = 'drive/MyDrive/ml4science'\n","\n","# Specify the directory for audio files\n","audio_directory = f'{base_path}/wavs/'\n","\n","# Specify the directory for the JSON files\n","json_directory = f'{base_path}/json/'\n","\n","# List all files in the audio directory and sort them\n","audio_files = sorted([f for f in os.listdir(audio_directory) if os.path.isfile(os.path.join(audio_directory, f))])\n","\n","# Printing the number of audios files\n","print(len(audio_files))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTUHZ4SKXCKM"},"outputs":[],"source":["# Iterate through each audio file - V100\n","# for audio_file in audio_files[0:29]: # - 1h19 min 55 s\n","# for audio_file in audio_files[29:58]: # - 1h09 min 11 s\n","# for audio_file in audio_files[58:87]: # - 1h11 min 33 s\n","# for audio_file in audio_files[87:116]: # - 1h13 min 48 s\n","\n","for audio_file in audio_files :\n","\n","    # Check if the file has a valid audio file extension\n","    valid_audio_extensions = ['.wav']  # Add more extensions if needed\n","    if not any(audio_file.lower().endswith(ext) for ext in valid_audio_extensions):\n","        continue  # Skip non-audio files\n","\n","    # Create the full path for the current audio file\n","    audio_file_path = os.path.join(audio_directory, audio_file)\n","\n","    # Get the filename (without extension)\n","    file_name = os.path.splitext(os.path.basename(audio_file))[0]\n","\n","    # Specify the JSON file path\n","    json_file_path = os.path.join(json_directory, f'transcript_{file_name}.json')\n","\n","    # Print the current file being processed\n","    print(f\"Processing: {audio_file}\")\n","\n","    # Perform transcription and alignment for the current audio file\n","    result = transcribe_audio_diarization(device=device, audio_file=audio_file_path, batch_size=batch_size)\n","\n","    # Write the result to the JSON file\n","    with open(json_file_path, 'w') as jsonfile:\n","        json.dump(result, jsonfile, indent=2)\n","\n","    # Printing when the current file as been processed\n","    print(f\"Done Processing: {audio_file}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"V100","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
