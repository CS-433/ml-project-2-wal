# Explaining venture teamsâ€™ opportunity identification through multimodal data

*Authors : Alessio Desogus, Haoyu Wen and Jiewei Li*

> **_ðŸ”” IMPORTANT NOTE:_** Per agreement with the [ENTC](https://www.epfl.ch/labs/entc/) supervisor of this project, *Dr. Davide Bavato*, a non-disclosure agreement (NDA) has been signed. Therefore, for maximum confidentiality, we will only provide the smaller amount of data possible in this repository. However, if it is **strictly necessary** for grading purposes, we could find a way to provide them.

## Abstract ðŸ“

REPORT ABSTRACT HERE

## Setup âš™ï¸



### WhisperX Setup :
- To install `WhisperX`, all the instructions can be found [here](https://github.com/m-bain/whisperX/blob/main/README.md). 
-  Before running (for reproducing our results) the `transcribing.ipynb` be sure to have added you personal [hugging face](https://huggingface.co) token `YOUR_HF_TOKEN`: 
```bash
diarize_model = whisperx.DiarizationPipeline(use_auth_token=YOUR_HF_TOKEN, device=device)
```
- To run the `transcribing.ipynb` notebook on [Google Colab](https://colab.research.google.com) as we did for more computational power, don't forget to run the following cell: 
```bash
# Import the Google Drive
from google.colab import drive
drive.mount('/content/drive')
```

```bash
pip install git+https://github.com/m-bain/whisperx.git
```

### Python Libraries Setup : 
- For this project, in addition to `WhisperX` and the basics `pandas`, `numpy`, `matplotlib` and others libraries, we used the following specific Python libraries that can be directly used on [Google Colab](https://colab.research.google.com) or installed in a [conda](https://docs.conda.io/projects/conda/en/latest/commands/install.html) environment locally :
1) `nltk`: natural language processing (NLP) 
```bash
conda install -c anaconda nltk
```
2) `statsmodels`: negative binomial regression model
```bash
conda install -c anaconda statsmodels
```
3) `scikit-learn`: machine learning models and functions
```bash
conda install -c anaconda scikit-learn
```

### Files Path Setup : 
- For each notebook, the only thing necessary for running it, is to add your base file path (from the Google Drive if running on [Google Colab](https://colab.research.google.com) or the local one) at the following command:
```bash
base_path = 'drive/MyDrive/...'  # on Google Colab
base_path = '.'                  # locally
```

## Data Origin and Generation ðŸ”
- Data from the [ENTC](https://www.epfl.ch/labs/entc/) : `wavs`, `jsons`, `rttms` and `dataset.csv`

- Data generated by us : `json`, `transcripts_teams.csv`, `transcripts_speakers.csv`, `speaking_time.csv` and `regression.txt`

> **_ðŸ“Œ NOTE:_** Only the `.csv` files and `.txt` are provided in this repository for confidentiality purposes.

## Data Input-Ouput Matrix Overview ðŸ”„


| File name             | File Input Directory | File Input Description | File Output Directory | File Output Description | 
| --------------        | ----------     | ----------   | ---------- | ---------- |
| `transcribing.ipynb`     | `/wavs` | Audio file of each team meeting `.wav` | `/json`| Diarized transcript from *WhisperX* `.json`           
| `cleaning.ipynb`    | `/json`| Diarized transcript from *WhisperX*  `.json` | `/csv` | `transcripts_teams.csv`: Organized at the team level, this dataset is crafted for team-specific analyses. It provides the team identification numbers, initial transcripts, filtered transcripts, and final clean transcripts.       
| `cleaning.ipynb`    | `/json`| Diarized transcript from *WhisperX* `.json` | `/csv` | `transcripts_speakers.csv`: Organized at the speaker level is created for speaker-specific analyses. It offers a view of individual speaker contributions.
| `cleaning.ipynb`    | `/json`| Diarized transcript from *WhisperX* `.json` | `/csv` | `speaking_time.csv`: Provides a temporal perspective on team speakers continuous speaking duration.          
| `main.ipynb`    | `/csv` | `dataset.csv`, `transcripts_teams.csv`, `transcripts_speakers.csv`, `speaking_time.csv` |Â cell output |Â `WhisperX Benchmark`, `Negative Binomial Regression` Results and `Classification` Results
| `main.ipynb`    | `/jsons` | Diarized transcript from *Deepgram* `.json` |Â cell output | `Deepgram Benchmark`Â 
| `main.ipynb`    | `/rttms` | Diarized transcript from *Pyannote* `.rttm` |Â cell output | `Pyannote Benchmark`     


## Audio Transcription and Diarization `transcribing.ipynb`

> **_ðŸš€ BEST PRACTICE:_**  If you want to reproduce the work done, the best practice is to run the `transcribing.ipynb` on [Google Colab](https://colab.research.google.com) as it needs some computational power.  

- We used approximately 50 units of computational power from [Google Colab](https://colab.research.google.com) with a [Tesla V100 GPU](https://colab.research.google.com/github/d2l-ai/d2l-tvm-colab/blob/master/chapter_gpu_schedules/arch.ipynb#scrollTo=PyGInfembT2s), and it took us approximately 5 hours to transcript and diarize the 116 audios files with `WhisperX`.

## Transcript Processing and Cleaning `processing.ipynb`

> **_ðŸ“Œ NOTE:_** This notebook takes approximately 2 minutes to run in its entirety, it can be done locally.

- **Description of the dataset:** `transcripts_teams.csv` ðŸ“œ

| Column                    | Description                                              |
|---------------------------|----------------------------------------------------------|
| `team_id`                 | Id number of the team (name of the audio file)           |
| `initial_transcript`      | The initial transcript from whisperX without any modifications |
| `filtered_transcript`     | The filtered transcript containing only words with a confidence level > 0.5 |
| `clean_final_transcript`  | The final clean transcript filtered with the `clean_text()` function |

- **Description of the dataset:** `transcripts_speakers.csv` ðŸŽ™ï¸

| Column                     | Description                                              |
|----------------------------|----------------------------------------------------------|
| `speaker_id`               | Id number of the team speaker                             |
| `speaker_initial_transcript`| Initial transcript segments of the speaker               |
| `speaker_filtered_transcript`| Filtered transcript segments of the speaker with a confidence level > 0.5 |
| `speaker_clean_final_transcript` | Final clean transcript segments of the speaker filtered with the `clean_text()` function |

- **Description of the dataset:** `speaking_time.csv` âŒ›

| Column   | Description                                          |
|----------|------------------------------------------------------|
| `Team_id`| Id number of the team (name of the file)             |
| `speaker`| Id number of the team speaker                        |
| `length` | Continuous speaking time for each speaker in seconds |


## Main Notebook Overview `main.ipynb`
> **_ðŸ“Œ NOTE:_** This notebook takes approximately 5 minutes to run in its entirety, it can be done locally.

- This is the main notebook which serves as the central hub for all the results presented in the report. To enhance clarity and maintain modular code, we've also included a helper file (`helpers.py`) that houses various functions used in `main.ipynb`.

The main structure of the notebook is as follows:

1. **Parameters Initialization**
2. **Idea 1: Uniqueness of Information through TF-IDF**
3. **Speaker Diarization Error Rate (SDER)**
4. **Idea 2: Speaking Time Concentration**
5. **Negative Binomial Regression as Prediction Model**
6. **Features Importance trough Random Forest Classification**
7. **Appendix: Gaussian Mixture Model (GMM)**

Feel free to navigate through the sections to explore specific analyses and findings. ðŸš€
















